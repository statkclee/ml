---
layout: page
title: xwMOOC 기계학습
subtitle: 나무기반 기계학습모형
output:
  html_document: 
    toc: yes
    hightlight: tango 
  pdf_document:
    latex_engine: xelatex
mainfont: NanumGothic
---
 

``` {r, include=FALSE}
source("tools/chunk-options.R")
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE)

library(tidyverse)
library(skimr)
library(AER)        # 카드발급 데이터가 포함됨 
library(rpart)      # 재귀분할 나무모형
library(caret)      # 예측모형 플랫폼
library(Metrics)    # 예측모형 성능평가


```

## 1. 나무기반 모형 {#tree-based-machine-leanring-model}

나무기반 모형은 통계전문가 뿐만 아니라, 컴퓨터 과학자, 사업하시는 분들에게서 사랑받는 가장 대중적인 기계학습 모형중에 하나다.
단점이 없는 모형이 없지만, 나무기반 모형은 다음 측면에서 다른 모형에 비해서 비교우위에 있는 것은 분명하다.

- 해석용이성
- 사용하기 편리함
- 정확성
- 결측값 처리 불필요

예측모형의 성능은 분산(variance)과 편향(bias)를 모두 줄여야 하는데 이를 위해서 나무기반 강력한 모형이 수십년동안 개발되었다.

- 재귀적 분할(recursive partitioning): `rpart`
- 배깅 (Bagging, **B**ootstrap **AGG**regat**ING**): `ipred`
- 랜덤 포리스트(Random Forest): `randomForest`
- GBM(Gradient Boosting Machine): `gbm`  

<img src="fig/ml-tree-overview.png" alt="나무기반 기계학습 모형 개요" width="100%" />

## 2. 재귀적 분할(recursive partitioning) {#tree-based-model-rpart}

재귀적 분할 예측모형 개발을 위해서 사전에 몇가지 팩키지를 준비한다.

- tidyverse
- skimr
- rpart      : 재귀분할 나무모형
- caret      : 예측모형 플랫폼
- [Metrics](https://github.com/mfrasco/Metrics)    : 예측모형 성능평가

UCI 기계학습 저장소에서 신용승인 데이터 [Credit Approval Data Set ](http://archive.ics.uci.edu/ml/datasets/credit+approval)을 다운로드 받아
신용승인 변수를 예측변수로 설정하고 연속형 변수 다수를 변환시킨 후에 `caret` 팩키지를 사용해서 데이터를 훈련 데이터와 검증 데이터로 나누고 나서, 재귀분할 나무 모형에 적합시킨다.

``` {r tree-based-model-rpart}
# 0. 환경설정 -----------------
# library(tidyverse)
# library(skimr)
# library(AER)        # 카드발급 데이터가 포함됨 
# library(rpart)      # 재귀분할 나무모형
# library(caret)      # 예측모형 플랫폼
# library(Metrics)    # 예측모형 성능평가
# library(ipred)      # 배깅(Bagging)

# 1. 데이터 가져오기 ----------

data("GermanCredit") # caret 팩키지 내장

# 2. 모형데이터 준비 ----------

split_index <- sample(1:3, size=nrow(GermanCredit), prob=c(0.7, 0.15, 0.15), replace = TRUE)

cc_train <- GermanCredit[split_index ==1,]
cc_valid <- GermanCredit[split_index ==2,]
cc_test  <- GermanCredit[split_index ==3,]

# 3. 재귀분할 나무모형 ----------

cc_rpart <- rpart(formula = Class ~ ., 
                      data = cc_train, 
                      method = "class",
                      parms = list(split = "gini"))

# 4. 재귀분할 나무모형 평가 ----------
## 4.1. 예측
cc_rpart_pred <- predict(object = cc_rpart, 
                 newdata = cc_test,
                 type = "class")

## 4.2. 예측평가
ce(actual = cc_test$Class, 
   predicted = cc_rpart_pred)

## 4.3. 재귀분할 나무모형 시각화
rpart.plot::rpart.plot(x = cc_rpart, yesno = 2, type = 0, extra = 0)
```

### 2.1. 재귀분할 나무모형 초모수(Hyper Parameter) {#tree-based-model-rpart-tuning}

재귀분할 나무모형에서 초모수로 `cp`, 복잡성모수(Complexity Parameter)를 선택하여 나무 가지치기(`prune`) 과정을 거쳐서 
최적의 모형을 구축하고 모형의 성능과 모형을 시각화하여 시각적 검증도 병행한다.

``` {r rpart-best-model}
# 5. 재귀분할 나무모형 초모수 설정 ---------------
## 5.1. CP 선정
optim_cp_key <- which.min(cc_rpart$cptable[, "xerror"])
optim_cp_value <- cc_rpart$cptable[optim_cp_key, "CP"]

## 5.2. 최적 재귀분할 나무모형 
cc_rpart_opt <- prune(tree = cc_rpart, 
                         cp = optim_cp_value)

## 5.3. 최적 재귀분할 나무모형 성능
cc_rpart_opt_pred <- predict(object = cc_rpart_opt, 
                         newdata = cc_test,
                         type = "class")

ce(actual = cc_test$Class, 
   predicted = cc_rpart_opt_pred)

## 5.4. 최적 재귀분할 나무모형 시각화
# rpart.plot::rpart.plot(x = cc_rpart_opt, yesno = 2, type = 0, extra = 0)
rattle::fancyRpartPlot(cc_rpart_opt)
```

### 2.1. 초모수 격자 검색(Hyper Parameter Grid Search) {#tree-based-model-grid-search}

재귀분할 의사결정 나무 모형에서 탐색할 초모수(hyper parameter)를 선정하고 이를 `expand.grid` 함수로 정의한다.

``` {r tree-based-grid-search-setup}
# 2. 격자 검색 ----------
## 2.1. 모수 설정

split_criterion <- c("gini", "information")
min_split <- seq(1, 4, 1)
max_depth <- seq(1, 6, 1)

hyper_grid <- expand.grid(min_split = min_split, 
                          max_depth = max_depth,
                          split_criterion = split_criterion)
```

### 2.2. 격자 검색 초모수 적합 {#tree-based-model-grid-search-fit}

두번째 단계로 각 초모수 조합 각각에 대해서 재귀분할 의사결정모형에 적합을 시킨다.

``` {r tree-based-grid-search-fit}
## 2.2. 모수 설정 적합
rpart_hyper_models <- list()

for (i in 1:nrow(hyper_grid)) {
  
  split_criterion <- hyper_grid$split_criterion[i]
  min_split       <- hyper_grid$min_split[i]
  max_depth       <- hyper_grid$max_depth[i]
  
  rpart_hyper_models[[i]] <- rpart(formula = Class ~ ., 
                             data = cc_train, 
                             method = "class",
                             minsplit = min_split,
                             maxdepth = max_depth,
                             parms = list(split = split_criterion))
}
```

### 2.3. 격자 검색 초모수 선정 {#tree-based-model-grid-search-optimal}

두번째 단계에서 적합된 모형 결과에서 정합성(accuracy)만을 뽑아 벡터로 저장해서 최적의 모형을 선정한다.

``` {r tree-based-grid-search-optimal}

# 3. 격자 검색 모형 평가 ----------

acc_values <- vector("numeric", length(rpart_hyper_models))

for(i in 1:length(rpart_hyper_models)) {

  pred <- predict(object = rpart_hyper_models[[i]],
                  newdata = cc_valid, type="class")
  
  acc_values[i] <- Metrics::ce(actual = cc_valid$Class, 
                               predicted = pred)
}

rpart_best_model <- rpart_hyper_models[[which.min(acc_values)]]

rpart_best_model$control
```

### 2.4. 최종 양산모형 {#tree-based-model-grid-search-in-production}

최적 모수 조합을 최종 모형으로 선정하고 이를 양산에 반영한다.

``` {r tree-based-grid-search-in-production}
# 4. 최종 양산 모형 ----------
best_pred <- predict(object = rpart_best_model,
                newdata = cc_test, type="class")

Metrics::ce(actual = cc_test$Class, 
            predicted = best_pred)

rpart.plot::rpart.plot(x = rpart_best_model, yesno = 2, type = 0, extra = 0)
```
